{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import seaborn as sns # boxplot\n",
    "from sklearn.preprocessing import normalize\n",
    "import random\n",
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, r2_score, confusion_matrix, accuracy_score, mean_squared_error, f1_score\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ``` Standardidation outlier k valo handle kore. ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression\n",
    "### normalization\n",
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression\n",
    "### standardadize\n",
    "### MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardadize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNClasifier:\n",
    "    def __init__(self,k):\n",
    "        self.k=k\n",
    "\n",
    "    def calcuEuclideanDistance(self,x1,x2,y1,y2):\n",
    "      return math.sqrt(math.pow((x1-x2),2)+math.pow((y1-y2),2))\n",
    "\n",
    "    def distanceCalculate(self,x_test):\n",
    "      distanceData=[]\n",
    "      for i in range(0,len(x_test)):\n",
    "        dist=[]\n",
    "        for j in range(0,len(self.x_train)):\n",
    "          dist.append(self.calcuEuclideanDistance(self.x_train.iloc[j,0],x_test.iloc[i,0],self.x_train.iloc[j,1],x_test.iloc[i,1]))\n",
    "        distanceData.append(dist)\n",
    "      self.distanceData_df = pd.DataFrame(data=distanceData)\n",
    "\n",
    "    def dataProcess(self,x_test):\n",
    "      tupleList=[]\n",
    "      for i in range(0,len(x_test)):\n",
    "        aa=[]\n",
    "        for j in range(0,len(self.x_train)):\n",
    "          aa.append((self.distanceData_df.iloc[i,j], self.y_train[j]))\n",
    "        aa.sort(key = lambda x:x[0])\n",
    "        # aa.sort()\n",
    "        tupleList.append(aa)\n",
    "      self.neighbour=pd.DataFrame(data=tupleList)\n",
    "\n",
    "    def neighbour(self):\n",
    "      # self.neighbours=self.neighbour.iloc[:,:self.k]\n",
    "      self.neighbours=self.neighbour.iloc[:,:self.k]\n",
    "\n",
    "    def assign(self):\n",
    "      assClass=[]\n",
    "      for i in range(0,len(self.neighbours[0])):\n",
    "          zeros=0\n",
    "          ones=0\n",
    "          for j in range(self.k):\n",
    "              if self.neighbours.iloc[i][j][1]==0:\n",
    "                  zeros+=1\n",
    "              else:\n",
    "                  ones+=1\n",
    "          # print(\"zero:\",zeros,\"one:\",ones)\n",
    "          if zeros>ones:\n",
    "              assClass.append(0)\n",
    "          else:\n",
    "              assClass.append(1)\n",
    "      self.assignedClass=assClass\n",
    "      # print(self.assignedClass)\n",
    "\n",
    "    def fit(self,x_train,y_train):\n",
    "      self.x_train=x_train\n",
    "      self.y_train=y_train\n",
    "    \n",
    "    def predict(self,x_test):\n",
    "      self.distanceCalculate(x_test)\n",
    "      self.dataProcess(x_test)\n",
    "      self.neighbour()\n",
    "      self.assign()\n",
    "      return self.assignedClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimise k value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accDataForOptimumKValue=[]\n",
    "for k in range(1,len(y_train)):\n",
    "    model=KNNClasifier(k)\n",
    "    model.fit(x_train,y_train)\n",
    "    y_predict=model.predict(x_test)\n",
    "    accDataForOptimumKValue.append((k,accuracy(y_predict,y_test)))\n",
    "\n",
    "accDataForOptimumKValue.sort(key=lambda x:x[1])\n",
    "print(\"The optimum k Value is : \", accDataForOptimumKValue[-1][0], \"and the accuracy is : \", accDataForOptimumKValue[-1][1]*100 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kmeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove the outer loop i.e. <b>for k in range(1,z):</b> if the question does not require wcss values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardadize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kmeans qith wcss, silhouette_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# z=int(math.sqrt(sampleSize))\n",
    "z=11\n",
    "threshold=float(input(\"enter threshold value: \"))\n",
    "\n",
    "wcssPlot=[]\n",
    "coeff=[]\n",
    "\n",
    "for k in range(1,z):\n",
    "  # random centroid choose\n",
    "  centroidPoint=[]\n",
    "  for i in range(k):\n",
    "    val=random.randint(0,sampleSize-1)\n",
    "    x=(x_scaler.iloc[val,0],x_scaler.iloc[val,1])\n",
    "    centroidPoint.append(x)\n",
    "  \n",
    "  while True:\n",
    "    newCentroid=[]\n",
    "    distanceMatrix=[]\n",
    "    for i in centroidPoint:\n",
    "      tempDist=[]\n",
    "      for j in range(sampleSize):\n",
    "        tempDist.append(calcuEuclideanDistance(i[0],x_scaler.iloc[j,0],i[1],x_scaler.iloc[j,1]))\n",
    "      distanceMatrix.append(tempDist)\n",
    "    distanceMatrix_df=pd.DataFrame(data=distanceMatrix)\n",
    "  \n",
    "  \n",
    "    assignedCluster=[]\n",
    "    for i in range(sampleSize):\n",
    "      assignedCluster.append((x_scaler.iloc[i,0],x_scaler.iloc[i,1],np.array(distanceMatrix_df.iloc[:,i]).argmin()))\n",
    "    assignedCluster=pd.DataFrame(assignedCluster)\n",
    "  \n",
    "  \n",
    "    for i in range(len(centroidPoint)):\n",
    "      x_temp=[]\n",
    "      y_temp=[]\n",
    "      for j in range(sampleSize):\n",
    "        if assignedCluster.iloc[j,2] == i:\n",
    "          x_temp.append(x_scaler.iloc[j,0])\n",
    "          y_temp.append(x_scaler.iloc[j,1])\n",
    "      x_mean=statistics.mean(x_temp)\n",
    "      y_mean=statistics.mean(y_temp)\n",
    "      newCentroid.append((x_mean,y_mean))\n",
    "  \n",
    "    centroidPoint_df=pd.DataFrame(centroidPoint)\n",
    "    newCentroid_df=pd.DataFrame(newCentroid)\n",
    "  \n",
    "    dist=[]\n",
    "    for i in range(k):\n",
    "      dist.append(calcuEuclideanDistance(centroidPoint_df.iloc[i,0],newCentroid_df.iloc[i,0],centroidPoint_df.iloc[i,1],newCentroid_df.iloc[i,1]))\n",
    "      \n",
    "    flag=True\n",
    "    for i in dist:\n",
    "      if i>threshold:\n",
    "        flag=False\n",
    "        break\n",
    "    if flag == True:\n",
    "      break\n",
    "    centroidPoint=newCentroid\n",
    "  \n",
    "  # k-means-end\n",
    "  nc=pd.DataFrame(newCentroid)\n",
    "  ac=pd.DataFrame(assignedCluster)\n",
    "\n",
    "  # wcss\n",
    "  wcss=0\n",
    "  for i in range(k+1):\n",
    "    for j in range(sampleSize):\n",
    "      if ac.iloc[j,2] == i:\n",
    "        wcss = wcss + calcuEuclideanDistance(ac.iloc[j,0],nc.iloc[i,0],ac.iloc[j,1],nc.iloc[i,1])\n",
    "  wcssPlot.append((wcss,k))\n",
    "  # silhouette_coefficient\n",
    "  coeff.append((silhouette_coefficient(ac.iloc[:,:2],ac.iloc[:,2]),k))\n",
    "\n",
    "print(\"WCSS values : \",wcssPlot)\n",
    "print(\"coefficient values : \",coeff)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## silhouette coefficient function\n",
    "X = DataFrame of features\n",
    "labels = array of features name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def silhouette_coefficient(X, labels):\n",
    "\n",
    "    n_samples = len(X)\n",
    "    unique_labels = np.unique(labels)\n",
    "    n_clusters = len(unique_labels)\n",
    "\n",
    "    # Calculate pairwise distance matrix\n",
    "    distances = pairwise_distances(X)\n",
    "\n",
    "    # Calculate average intra-cluster distances\n",
    "    avg_intra_distances = np.zeros(n_samples)\n",
    "    for i in range(n_clusters):\n",
    "        mask = labels == unique_labels[i]\n",
    "        avg_intra_distances[mask] = np.mean(distances[mask][:, mask], axis=1)\n",
    "\n",
    "    # Calculate average inter-cluster distances\n",
    "    avg_inter_distances = np.zeros(n_samples)\n",
    "    for i in range(n_clusters):\n",
    "        mask = labels != unique_labels[i]\n",
    "        avg_inter_distances[mask] += np.sum(distances[mask][:, labels == unique_labels[i]], axis=1)\n",
    "        avg_inter_distances[mask] /= np.sum(labels == unique_labels[i])\n",
    "\n",
    "    # Calculate silhouette coefficients for each sample\n",
    "    silhouette_scores = (avg_inter_distances - avg_intra_distances) / np.maximum(avg_inter_distances, avg_intra_distances)\n",
    "\n",
    "    # Calculate the mean silhouette coefficient\n",
    "    mean_silhouette = np.mean(silhouette_scores)\n",
    "\n",
    "    return mean_silhouette"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standradadize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PCA object\n",
    "pca = decomposition.PCA()\n",
    "# Fit the PCA object to the data\n",
    "pca.fit(X)\n",
    "# Get the principal components\n",
    "principal_components = pca.components_\n",
    "# Project the data onto the principal components\n",
    "X_reduced = pca.transform(X)\n",
    "# Print the principal components\n",
    "print(principal_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48f2edc3205953217ede649613258eee76e5ed51b138be8ebb044f87332c9625"
  },
  "kernelspec": {
   "display_name": "Python 3.10.11 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
